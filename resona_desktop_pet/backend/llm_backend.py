import json
import re
from datetime import datetime
from typing import Optional, Callable, Any, List
from pathlib import Path
from dataclasses import dataclass, field

from ..config import ConfigManager


@dataclass
class LLMResponse:
    """Structured response from LLM."""
    emotion: str = "<E:smile>"
    text_display: str = ""
    text_tts: str = ""
    thought: str = ""
    raw_response: str = ""
    error: Optional[str] = None


@dataclass
class ConversationHistory:
    """Manages conversation history in standard message format."""
    max_rounds: int = 14
    history: list = field(default_factory=list)

    def add(self, role: str, content: str) -> None:
        """Add a message to history."""
        self.history.append({"role": role, "content": content})
        # Keep only max_rounds * 2 (user + assistant)
        limit = self.max_rounds * 2
        if len(self.history) > limit:
            self.history = self.history[-limit:]

    def get_messages(self) -> list:
        """Get the history as a list of messages."""
        return self.history

    def clear(self) -> None:
        """Clear all history."""
        self.history.clear()


class LLMBackend:
    """LLM Backend supporting persistent cloud clients and dedicated logging."""

    def __init__(self, config: ConfigManager, log_path: Optional[Path] = None):
        self.config = config
        self.log_path = log_path
        self.history = ConversationHistory(max_rounds=config.max_rounds)
        
        # Persistent clients
        self._openai_client = None
        self._claude_client = None
        self._gemini_safety = {}
        self._active_model_name = None
        
        # Initial connect
        self.reconnect()

    def reconnect(self):
        """Pre-initialize LLM clients based on current config."""
        llm_cfg = self.config.get_llm_config()
        model_type = llm_cfg["model_type"]
        model_name = llm_cfg["model_name"]
        api_key = llm_cfg["api_key"]
        base_url = llm_cfg.get("base_url", "")
        
        print(f"[LLM] Initializing persistent client for: {model_name}")

        try:
            if model_type == 5:
                # Import Google Generative AI only if needed
                import google.generativeai as genai
                from google.generativeai.types import HarmCategory, HarmBlockThreshold
                
                genai.configure(api_key=api_key)
                self._gemini_safety = {
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            
            elif model_type == "local" or model_type in [1, 2, 4, 6]:
                from openai import AsyncOpenAI
                self._openai_client = AsyncOpenAI(api_key=api_key, base_url=base_url)
                
            elif model_type == 3:
                import anthropic
                self._claude_client = anthropic.AsyncAnthropic(api_key=api_key)
                
            self._active_model_name = model_name
            print(f"[LLM] Client metadata initialized.")
        except Exception as e:
            print(f"[LLM] Failed to pre-initialize client: {e}")

    def _get_precise_time_context(self) -> str:
        """Get detailed current time information."""
        now = datetime.now()
        weekday_map = ["星期一", "星期二", "星期三", "星期四", "星期五", "星期六", "星期日"]
        time_str = now.strftime("%Y-%m-%d %H:%M:%S")
        weekday = weekday_map[now.weekday()]
        
        hour = now.hour
        if 0 <= hour < 5: period = "深夜"
        elif 5 <= hour < 8: period = "凌晨"
        elif 8 <= hour < 11: period = "早上"
        elif 11 <= hour < 13: period = "中午"
        elif 13 <= hour < 17: period = "下午"
        elif 17 <= hour < 19: period = "傍晚"
        elif 19 <= hour < 23: period = "晚上"
        else: period = "深夜"
        
        return f"当前本地时间: {time_str} ({weekday}, {period})"

    def _build_messages(self, question: str, provider: str = "default") -> list:
        """Build message list for LLM API with provider-specific history cleaning."""
        messages = []

        # 1. System prompt (Core rules + Time Context)
        system_prompt = self.config.get_prompt()
        
        if self.config.enable_time_context:
            time_info = self._get_precise_time_context()
            system_prompt += f"\n\n[Context Info]\n{time_info}"
        
        context_instruction = (
            "\n2. 你拥有完整的对话上下文记忆，请结合历史记录进行连贯的回答。"
        )
        messages.append({"role": "system", "content": system_prompt + context_instruction})

        # 2. History
        raw_history = self.history.get_messages()
        for msg in raw_history:
            content = msg["content"]
            # SPECIAL CLEANING: Remove JSON structure from assistant messages to avoid model confusion
            if msg["role"] == "assistant":
                try:
                    if content.strip().startswith("{"):
                        data = json.loads(content)
                        content = data.get("text_display", content)
                except:
                    pass 
            messages.append({"role": msg["role"], "content": content})

        # 3. Current Question
        messages.append({"role": "user", "content": question})
        return messages

    def _parse_response(self, text: str) -> LLMResponse:
        """Parse LLM response JSON with robust extraction."""
        response = LLMResponse(raw_response=text)
        print(f"[LLM] Raw response from API: {text[:200]}...")

        # Clean markdown code blocks
        json_str = text.strip()
        if "```" in json_str:
            match = re.search(r"```(?:json)?\s*(.*?)\s*```", json_str, re.DOTALL)
            if match:
                json_str = match.group(1).strip()
        
        # If still not JSON, try finding first curly brace
        if not json_str.startswith("{"):
            match = re.search(r"({.*})", json_str, re.DOTALL)
            if match:
                json_str = match.group(1).strip()

        try:
            data = json.loads(json_str)
            response.emotion = data.get("emotion", "<E:smile>")
            response.text_display = data.get("text_display", "")
            response.text_tts = data.get("text_tts", response.text_display)
            return response
        except json.JSONDecodeError as e:
            print(f"[LLM] JSON Parse Error: {e} | Candidate: {json_str[:100]}...")
            response.error = f"JSON parsing failed: {str(e)}"
            return response

    def _log_interaction(self, request_data: Any, response_raw: str):
        """Write interaction to dedicated LLM log file and standard logger."""
        import logging
        llm_logger = logging.getLogger("LLM")
        
        if not self.log_path: return
        try:
            self.log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.log_path, "a", encoding="utf-8") as f:
                ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"\n={'='*20} {ts} {'='*20}\n")
                f.write(f"[REQUEST]\n{json.dumps(request_data, ensure_ascii=False, indent=2)}\n")
                f.write(f"[RESPONSE RAW]\n{response_raw}\n")
            
            # Summary to structured logger (llm.log via main.py setup)
            llm_logger.info(f"Interaction logged to {self.log_path.name}")
        except Exception as e:
            print(f"[LLM] Logging error: {e}")

    async def query_openai_compatible(
        self,
        question: str,
        model_name: str,
        temperature: float = 0.7,
        top_p: float = 1.0,
        max_tokens: int = 500
    ) -> LLMResponse:
        """Query OpenAI/DeepSeek compatible API."""
        try:
            if not self._openai_client: self.reconnect()
            messages = self._build_messages(question)
            self._log_interaction(messages, "WAITING...")
            
            response = await self._openai_client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )
            
            message = response.choices[0].message
            raw_text = message.content or ""
            reasoning = getattr(message, "reasoning_content", "")
            
            # Handle R1 style thinking tags in content
            for tag in ["think", "thinking"]:
                if f"<{tag}>" in raw_text:
                    pattern = rf"<{tag}>(.*?)</{tag}>"
                    match = re.search(pattern, raw_text, re.DOTALL)
                    if match:
                        reasoning += match.group(1).strip()
                        raw_text = re.sub(pattern, "", raw_text, flags=re.DOTALL).strip()

            log_content = f"[Reasoning]\n{reasoning}\n\n[Content]\n{raw_text}" if reasoning else raw_text
            self._log_interaction("DONE", log_content)
            
            if not raw_text: return LLMResponse(error="Empty response from LLM", thought=reasoning)

            llm_resp = self._parse_response(raw_text)
            llm_resp.thought = reasoning
            return llm_resp
        except Exception as e:
            self._log_interaction("EXCEPTION", str(e))
            return LLMResponse(error=str(e))

    async def query_gemini(
        self,
        question: str,
        temperature: float = 0.7,
        top_p: float = 1.0,
        max_tokens: int = 500
    ) -> LLMResponse:
        """Query Google Gemini with specialized context mapping."""
        try:
            import google.generativeai as genai
            messages = self._build_messages(question, provider="gemini")
            
            chat_history = []
            system_instruction = ""
            for msg in messages:
                if msg["role"] == "system": system_instruction = msg["content"]
                elif msg["role"] == "user": chat_history.append({"role": "user", "parts": [msg["content"]]})
                elif msg["role"] == "assistant": chat_history.append({"role": "model", "parts": [msg["content"]]})

            model = genai.GenerativeModel(
                model_name=self._active_model_name,
                safety_settings=self._gemini_safety,
                generation_config={"temperature": temperature, "top_p": top_p, "max_output_tokens": max_tokens}
            )
            
            current_query = chat_history.pop()["parts"][0]
            chat = model.start_chat(history=chat_history)
            # Prepend instruction to first message if history was empty
            if system_instruction and not chat_history:
                current_query = f"System Instruction: {system_instruction}\n\nUser Question: {current_query}"

            response = await chat.send_message_async(current_query)
            if not response.candidates: return LLMResponse(error="Empty response")
            
            raw_text = response.text
            thought_text = ""
            # Extract thinking tags if present
            for tag in ["think", "thinking"]:
                pattern = rf"<{tag}>(.*?)</{tag}>"
                match = re.search(pattern, raw_text, re.DOTALL)
                if match:
                    thought_text += match.group(1).strip()
                    raw_text = re.sub(pattern, "", raw_text, flags=re.DOTALL).strip()

            self._log_interaction("DONE", f"[Thought]\n{thought_text}\n\n[Text]\n{raw_text}")
            llm_resp = self._parse_response(raw_text)
            llm_resp.thought = thought_text
            return llm_resp
        except Exception as e:
            self._log_interaction("EXCEPTION", str(e))
            return LLMResponse(error=str(e))

    async def query_claude(
        self,
        question: str,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> LLMResponse:
        """Query Anthropic Claude."""
        try:
            if not self._claude_client: self.reconnect()
            messages = self._build_messages(question, provider="claude")
            
            system_prompt = next((m["content"] for m in messages if m["role"] == "system"), "")
            filtered_messages = [m for m in messages if m["role"] != "system"]
            
            self._log_interaction(filtered_messages, "WAITING...")
            response = await self._claude_client.messages.create(
                model=model_name,
                system=system_prompt,
                messages=filtered_messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )

            final_text = "".join(block.text for block in response.content if block.type == "text")
            thought_text = "".join(block.thinking for block in response.content if block.type == "thinking")

            self._log_interaction("DONE", f"[Thought]\n{thought_text}\n\n[Text]\n{final_text}")
            llm_resp = self._parse_response(final_text)
            llm_resp.thought = thought_text
            return llm_resp
        except Exception as e:
            self._log_interaction("EXCEPTION", str(e))
            return LLMResponse(error=str(e))

    async def query(self, question: str) -> LLMResponse:
        """Consolidated LLM query entry point."""
        llm_config = self.config.get_llm_config()
        model_type = llm_config["model_type"]
        model_name = llm_config["model_name"]
        
        if model_name != self._active_model_name:
            self.reconnect()

        try:
            if model_type == "local" or model_type in [1, 2, 4, 6]:
                response = await self.query_openai_compatible(
                    question, model_name,
                    temperature=llm_config.get("temperature", 0.7),
                    top_p=llm_config.get("top_p", 1.0),
                    max_tokens=llm_config.get("max_tokens", 500)
                )
            elif model_type == 5:
                response = await self.query_gemini(
                    question,
                    temperature=llm_config.get("temperature", 0.7),
                    top_p=llm_config.get("top_p", 1.0),
                    max_tokens=llm_config.get("max_tokens", 500)
                )
            elif model_type == 3:
                response = await self.query_claude(
                    question, model_name,
                    temperature=llm_config.get("temperature", 0.7),
                    max_tokens=llm_config.get("max_tokens", 500)
                )
            else:
                response = LLMResponse(error=f"Unsupported model type: {model_type}")
        except Exception as e:
            response = LLMResponse(error=f"Request Failed: {e}")

        if not response.error and response.text_display:
            self.history.add("user", question)
            self.history.add("assistant", response.raw_response)

        return response

    def clear_history(self) -> None:
        """Clear conversation history."""
        self.history.clear()